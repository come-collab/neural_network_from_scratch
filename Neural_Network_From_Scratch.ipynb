{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "           ]\n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "layer_outputs= np.dot(weights,inputs) + biases\n",
    "print(layer_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrice of (3,4) for the inputs\n",
    "\n",
    "inputs = [  [1, 2, 3, 2.5],\n",
    "            [2., 5., -1., 2],\n",
    "            [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "\n",
    "#Matrice of (3,4) for the weight\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "layer 2 outputs [[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array(weights)\n",
    "inputs = np.array(inputs)\n",
    "layer1_outputs = np.dot(inputs,weights.T) + biases \n",
    "print(layer1_outputs)\n",
    "\n",
    "#To pass throught another layer we use layer1 as an output\n",
    "\n",
    "layer2_outputs = np.dot(layer1_outputs,np.array(weights2).T) + biases2\n",
    "print(\"layer 2 outputs\" , layer2_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Copyright (c) 2015 Andrej Karpathy\n",
    "# License: https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\n",
    "# Source: https://cs231n.github.io/neural-networks-case-study/\n",
    "def create_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dense layer class\n",
    "X,y = create_data(samples= 100, classes=3)\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.31453791 -1.90359528  0.45242016  1.20968038  0.54963469]\n",
      " [ 0.32752073 -1.05672248  0.82112911  0.94649061  0.08032959]]\n",
      "-0.4323063486215709\n",
      "-0.4323063486215709\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randn(2,5))\n",
    "weights_test = np.random.randn(2,5)\n",
    "mean = weights_test.mean()\n",
    "weights_test[0].mean()\n",
    "weights_test[1].mean()\n",
    "\n",
    "print((weights_test[0].mean() + weights_test[1].mean())/2)\n",
    "print(mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense Layer Class\n",
    "import numpy as np\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        #Initialize weights and biases dont forget here the order is because we \"pre Transpose\" them\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        print(self.weights)\n",
    "        \n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        pass  \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        #Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00227231  0.01311536 -0.00338114 -0.00584247]\n",
      " [ 0.00069858 -0.00110148 -0.00307301 -0.00014158]]\n",
      "[[0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 2\n",
    "n_neurons = 4\n",
    "\n",
    "weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "biases = np.zeros((1,n_neurons))\n",
    "\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00895608 0.00283096 0.0045413 ]\n",
      " [0.00197121 0.01379019 0.0003884 ]]\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.21424529e-06  1.32390325e-04 -3.94589354e-06]\n",
      " [ 1.17387644e-04  2.74313119e-04  4.85134735e-05]\n",
      " [ 1.44484949e-04  4.23128080e-04  5.57441352e-05]\n",
      " [ 2.99408026e-04  4.93119581e-04  1.33324261e-04]]\n"
     ]
    }
   ],
   "source": [
    "#Lets create a forward pass : \n",
    "\n",
    "X, y = create_data(samples=100,classes=3)\n",
    "dense1 = Layer_Dense(2,3)\n",
    "dense1.forward(X)\n",
    "print(dense1.output[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs= [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0, 2]\n",
      "[0, 2, 0]\n",
      "[0, 2, 0, 3.3]\n",
      "[0, 2, 0, 3.3, 0]\n",
      "[0, 2, 0, 3.3, 0, 1.1]\n",
      "[0, 2, 0, 3.3, 0, 1.1, 2.2]\n",
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs= [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output = np.maximum(0,inputs)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0211834   0.00357328  0.00810225]\n",
      " [-0.00535749  0.01475363  0.00582863]]\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.52605269e-04 8.25223460e-05]\n",
      " [0.00000000e+00 2.76172913e-04 8.06479212e-05]\n",
      " [0.00000000e+00 4.15156749e-04 2.91949949e-04]\n",
      " [0.00000000e+00 2.63882257e-04 3.68798405e-04]\n",
      " [0.00000000e+00 6.28956980e-04 5.01529103e-04]]\n"
     ]
    }
   ],
   "source": [
    "X, y = create_data(samples=100,classes=3)\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "print(activation1.output[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponnentiated values : \n",
      "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8,1.21,2.385]\n",
    "E = 2.71828182846\n",
    "\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "print(\"exponnentiated values : \")\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values : \n",
      "[0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n",
      "Sum of normalized values :  1.0\n"
     ]
    }
   ],
   "source": [
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print(\"Normalized exponentiated values : \")\n",
    "print(norm_values)\n",
    "print(\"Sum of normalized values : \", sum(norm_values))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values : \n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values : \n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values :  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Using Numpy it looks like this : \n",
    "\n",
    "import numpy as np\n",
    "layer_outputs = [4.8,1.21,2.385]\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponentiated values : ')\n",
    "print(exp_values)\n",
    "\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values : ')\n",
    "print(norm_values)\n",
    "print('sum of normalized values : ', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m exp_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(inputs)\n\u001b[1;32m----> 3\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m exp_values \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "exp_values = np.exp(inputs)\n",
    "\n",
    "probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis\n",
      "18.172\n",
      "18.054999999999996\n",
      "This will be identitcal to the above since default is None : \n",
      "18.172\n",
      "This will be the addition the axis = 1  [15.11   0.451  2.611]\n",
      "They should be identical :  [8.395 7.29  2.487]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "layer_outputs = np.array([\n",
    "    [4.8,1.21,2.385],\n",
    "    [8.9,-1.81,0.2],\n",
    "    [1.41,1.051,0.026]\n",
    "    ])\n",
    "print('Sum without axis')\n",
    "print(np.sum(layer_outputs))\n",
    "print(4.8 + 1.12 + 2.385 + 8.9 - 1.81 + 0.2 + 1.41 + 1.05)\n",
    "print('This will be identitcal to the above since default is None : ')\n",
    "print(np.sum(layer_outputs, axis=None))\n",
    "\n",
    "print('This will be the addition the axis = 1 ' , np.sum(layer1_outputs, axis = 0))\n",
    "print('They should be identical : ' , np.array([4.8 + 1.21 + 2.385, 8.9 -1.81 + 0.2,1.41+1.051+0.026 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis 1, but keep the same dimensions as input : \n",
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis 1, but keep the same dimensions as input : ')\n",
    "print(np.sum(layer_outputs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        probabilities= exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.outputs = probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "22026.465794806718\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.exp(1))\n",
    "print(np.exp(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  [[-0.          0.        ]\n",
      " [-0.00658074  0.00766318]\n",
      " [ 0.0126167   0.01577785]\n",
      " [-0.01006512  0.02858264]\n",
      " [ 0.0117842   0.03864737]]\n",
      "[[ 0.01249502  0.00467951 -0.00806445]\n",
      " [ 0.00785379 -0.00392767 -0.00861373]]\n",
      "[[-0.00514156  0.0137049  -0.00263028]\n",
      " [-0.002748    0.01895327 -0.02385485]\n",
      " [ 0.00780996  0.00582705  0.01422339]]\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333267 0.33333443 0.3333329 ]\n",
      " [0.3333331  0.33333372 0.33333318]\n",
      " [0.33333226 0.3333351  0.33333264]]\n"
     ]
    }
   ],
   "source": [
    "X, y = create_data(samples=100, classes=3)\n",
    "\n",
    "print(\"X : \" , X[:5])\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.outputs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333339 0.3333332  0.33333342]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333364 0.33333255 0.3333338 ]\n",
      " [0.33333348 0.33333296 0.33333356]]\n"
     ]
    }
   ],
   "source": [
    "#Get the full code up to this point + the mission to understand precisely why there is 33% chance\n",
    "#I get it : \n",
    "#Since the distribution of weights are initialized randomly\n",
    "# close to mean zero following standard distribution\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.output= np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probabilities\n",
    "\n",
    "#Create dataset \n",
    "\n",
    "X,y = create_data(samples = 100, classes=3)\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "#We are going to the loss function previous topics are understood \n",
    "\n",
    "softmax_output = [0.7,0.1,0.2]\n",
    "#lets say that thefirst class is the right one\n",
    "\n",
    "import math\n",
    "\n",
    "target_output = [1,0,0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
    "         math.log(softmax_output[1]) * target_output[1]+\n",
    "         math.log(softmax_output[2]) * target_output[2])\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.log(1.))\n",
    "print(math.log(0.95))\n",
    "print(math.log(0.9))\n",
    "print(math.log(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "b= 5.2\n",
    "print(np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.e ** 1.6486586255873816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array(\n",
    "    [\n",
    "        [0.7,0.1,0.2],\n",
    "        [0.1,0.5,0.4],\n",
    "        [0.02,0.9,0.08]\n",
    "    ]\n",
    ")\n",
    "\n",
    "class_targets = [0,1,1] #one hot encoded vector of a dog, cat,cat\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets,softmax_outputs):\n",
    "    print(distribution[targ_idx])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array(\n",
    "    [\n",
    "        [0.7,0.1,0.2],\n",
    "        [0.1,0.5,0.4],\n",
    "        [0.02,0.9,0.08]\n",
    "    ]\n",
    ")\n",
    "\n",
    "class_targets = [0,1,1] #one hot encoded vector of a dog, cat,cat\n",
    "\n",
    "print(softmax_outputs[[0,1,2],class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)),class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)),class_targets\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs [range(len(softmax_outputs)),class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "softmax_outputs = np.array(\n",
    "    [\n",
    "        [0.7,0.1,0.2],\n",
    "        [0.1,0.5,0.4],\n",
    "        [0.02,0.9,0.08]\n",
    "    ]\n",
    ")\n",
    "\n",
    "class_targets = np.array(\n",
    "    [\n",
    "        [1,0,0],\n",
    "        [0,1,0],\n",
    "        [0,1,0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Probabilities for target values\n",
    "#Only if categorical labels\n",
    "\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "    \n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs * class_targets,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "neg_log = np.mean(neg_log)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SESA733530\\AppData\\Local\\Temp\\ipykernel_8280\\2692069060.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  np.mean([1,2,3,-np.log(0)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.11809565095832"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([1,2,3,-np.log(0)])\n",
    "-np.log(1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_condidences = y_pred_clipped [range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs,class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.          0.        ]\n",
      " [-0.00278627  0.00970912]\n",
      " [ 0.01527156  0.01322501]\n",
      " [ 0.03004437 -0.0039509 ]\n",
      " [ 0.01026293  0.03907888]]\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333306 0.33333416 0.33333278]\n",
      " [0.3333328  0.33333495 0.33333224]\n",
      " [0.33333315 0.3333339  0.33333295]]\n",
      "loss 1.0986149729339745\n"
     ]
    }
   ],
   "source": [
    "#Everything until here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__ (self,n_inputs,n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "      \n",
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward (self,inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "        \n",
    "        probabilities = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "    def calculate (self,output,y):\n",
    "        \n",
    "        sample_losses = self.forward(output,y)\n",
    "        \n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    \n",
    "    def forward(self,y_pred,y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        \n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "            \n",
    "        elif len(y_true.shape) == 2 :\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,axis=1\n",
    "            )\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        \n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "X, y = create_data(samples=100, classes=3)\n",
    "print(X[:5])\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "dense1.forward(X)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"loss\" , loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "softmax_outputs = np.array(\n",
    "    [\n",
    "        [0.7,0.2,0.1],\n",
    "        [0.5,0.1,0.4],\n",
    "        [0.02,0.9,0.08]\n",
    "    ]\n",
    ")\n",
    "\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "\n",
    "\n",
    "if len(class_targets.shape) ==2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "    \n",
    "#Have to re do the prediction part not understanding it well\n",
    "if len(y.shape) ==2:\n",
    "    y = np.argmax(y,axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "accuracy = np.mean(predictions==class_targets)\n",
    "\n",
    "\n",
    "print(\"acc\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Modified from:\n",
    "# Copyright (c) 2015 Andrej Karpathy\n",
    "# License: https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\n",
    "# Source: https://cs231n.github.io/neural-networks-case-study/\n",
    "def create_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        X[ix] = np.c_[np.random.randn(samples)*.1 + (class_number)/3, np.random.randn(samples)*.1 + 0.5]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X,y = create_data(samples=100, classes=3)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,s=40,cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration 0 loss : 1.0986403427869682 acc 0.3333333333333333\n",
      "New set of weights found, iteration 2 loss : 1.0983980869732057 acc 0.3333333333333333\n",
      "New set of weights found, iteration 52 loss : 1.0979744676497627 acc 0.3333333333333333\n",
      "New set of weights found, iteration 203 loss : 1.0963033075676665 acc 0.3333333333333333\n",
      "New set of weights found, iteration 391 loss : 1.0961757013908777 acc 0.5666666666666667\n",
      "New set of weights found, iteration 7138 loss : 1.0957509393475746 acc 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Trying to have a good model by randomly assigning weights until the loss is very close to zero\n",
    "\n",
    "X, y = create_data(samples=100,classes=3)\n",
    "\n",
    "dense1=Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "lowest_loss = 999999\n",
    "\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(10000):\n",
    "    dense1.weights = 0.05 * np.random.randn(2,3)\n",
    "    dense1.biases = 0.05 * np.random.randn(1,3)\n",
    "    dense2.weights = 0.05 * np.random.randn(3,3)\n",
    "    dense2.biases = 0.05*np.random.randn(1,3)\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration', iteration, 'loss :', loss , 'acc', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set of weights found, iteration :  0 loss 1.0995240562425788 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  13 loss 1.0994602424334923 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  15 loss 1.0991929521360484 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  17 loss 1.0989221555899247 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  19 loss 1.0986392668122649 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  22 loss 1.0969890235450632 acc: 0.35\n",
      "New set of weights found, iteration :  25 loss 1.0967564655232807 acc: 0.33666666666666667\n",
      "New set of weights found, iteration :  28 loss 1.0955484541628568 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  33 loss 1.0951355357264594 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  34 loss 1.0913046329715883 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  36 loss 1.0856600698586039 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  37 loss 1.0850718900694323 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  39 loss 1.0841782159495739 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  44 loss 1.0796741012910003 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  45 loss 1.0785886042375228 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  46 loss 1.0758414951557103 acc: 0.5966666666666667\n",
      "New set of weights found, iteration :  47 loss 1.0719345497198838 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  48 loss 1.070984523725207 acc: 0.5333333333333333\n",
      "New set of weights found, iteration :  53 loss 1.0685177177307779 acc: 0.6133333333333333\n",
      "New set of weights found, iteration :  55 loss 1.067509191190636 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  58 loss 1.062292841200859 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  60 loss 1.0610898291877915 acc: 0.5233333333333333\n",
      "New set of weights found, iteration :  64 loss 1.0591533210718354 acc: 0.3333333333333333\n",
      "New set of weights found, iteration :  67 loss 1.055378629339178 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  73 loss 1.0519050162423618 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  74 loss 1.0468276799182383 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  77 loss 1.0431973780590227 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  79 loss 1.036160217018193 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  81 loss 1.0246866952824614 acc: 0.6533333333333333\n",
      "New set of weights found, iteration :  86 loss 1.0235878000166108 acc: 0.6566666666666666\n",
      "New set of weights found, iteration :  87 loss 1.0142400988306683 acc: 0.6566666666666666\n",
      "New set of weights found, iteration :  95 loss 1.0099845330334691 acc: 0.6433333333333333\n",
      "New set of weights found, iteration :  97 loss 1.0052676225669637 acc: 0.5933333333333334\n",
      "New set of weights found, iteration :  98 loss 1.0013198160319623 acc: 0.5866666666666667\n",
      "New set of weights found, iteration :  100 loss 0.9821278428717716 acc: 0.5633333333333334\n",
      "New set of weights found, iteration :  101 loss 0.9766586449388955 acc: 0.5533333333333333\n",
      "New set of weights found, iteration :  104 loss 0.9672380853616952 acc: 0.59\n",
      "New set of weights found, iteration :  105 loss 0.9663938566013208 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  106 loss 0.9594638929100218 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  108 loss 0.9546234596115111 acc: 0.6166666666666667\n",
      "New set of weights found, iteration :  110 loss 0.9422795314251804 acc: 0.65\n",
      "New set of weights found, iteration :  111 loss 0.9395288548791285 acc: 0.6\n",
      "New set of weights found, iteration :  115 loss 0.9312809496130292 acc: 0.64\n",
      "New set of weights found, iteration :  116 loss 0.9234172878922311 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  118 loss 0.923326999141838 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  119 loss 0.9007235051469136 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  122 loss 0.899145202757147 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  124 loss 0.8956988898663307 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  126 loss 0.8932445705337574 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  127 loss 0.8893092680693598 acc: 0.66\n",
      "New set of weights found, iteration :  128 loss 0.8861134100300052 acc: 0.6433333333333333\n",
      "New set of weights found, iteration :  131 loss 0.879785938651867 acc: 0.6533333333333333\n",
      "New set of weights found, iteration :  132 loss 0.8783472245700154 acc: 0.6233333333333333\n",
      "New set of weights found, iteration :  135 loss 0.8747473372763159 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  136 loss 0.8691306901351223 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  137 loss 0.8602644764278459 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  138 loss 0.8473164966047697 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  141 loss 0.8379907737569647 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  143 loss 0.8338385201978268 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  144 loss 0.8325974180194782 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  152 loss 0.8319530389509731 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  158 loss 0.831432115218511 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  160 loss 0.8247636657213959 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  163 loss 0.8247356917966996 acc: 0.6633333333333333\n",
      "New set of weights found, iteration :  166 loss 0.8149552273087511 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  172 loss 0.8137293977410129 acc: 0.69\n",
      "New set of weights found, iteration :  183 loss 0.7988848001146255 acc: 0.7\n",
      "New set of weights found, iteration :  185 loss 0.7895957149779962 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  188 loss 0.7828954992657687 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  192 loss 0.7820906329129246 acc: 0.7533333333333333\n",
      "New set of weights found, iteration :  196 loss 0.7699306100865514 acc: 0.6666666666666666\n",
      "New set of weights found, iteration :  201 loss 0.7657615652655794 acc: 0.7533333333333333\n",
      "New set of weights found, iteration :  203 loss 0.7623626951897368 acc: 0.7466666666666667\n",
      "New set of weights found, iteration :  205 loss 0.7579787937179003 acc: 0.89\n",
      "New set of weights found, iteration :  213 loss 0.7536058078242457 acc: 0.8933333333333333\n",
      "New set of weights found, iteration :  215 loss 0.7440533342104484 acc: 0.89\n",
      "New set of weights found, iteration :  218 loss 0.7379039528076248 acc: 0.8266666666666667\n",
      "New set of weights found, iteration :  220 loss 0.7288926355912139 acc: 0.9033333333333333\n",
      "New set of weights found, iteration :  221 loss 0.7236555394046735 acc: 0.9066666666666666\n",
      "New set of weights found, iteration :  223 loss 0.7138612038891063 acc: 0.83\n",
      "New set of weights found, iteration :  224 loss 0.7078418267381217 acc: 0.8866666666666667\n",
      "New set of weights found, iteration :  226 loss 0.7069139949527575 acc: 0.7766666666666666\n",
      "New set of weights found, iteration :  229 loss 0.6927084460904954 acc: 0.7866666666666666\n",
      "New set of weights found, iteration :  230 loss 0.6803369051704434 acc: 0.77\n",
      "New set of weights found, iteration :  231 loss 0.6654101238407067 acc: 0.7866666666666666\n",
      "New set of weights found, iteration :  234 loss 0.6605024505421204 acc: 0.8866666666666667\n",
      "New set of weights found, iteration :  237 loss 0.6533893220562293 acc: 0.76\n",
      "New set of weights found, iteration :  238 loss 0.6478553649723253 acc: 0.6866666666666666\n",
      "New set of weights found, iteration :  240 loss 0.6345444819765744 acc: 0.7033333333333334\n",
      "New set of weights found, iteration :  242 loss 0.6304802564514653 acc: 0.77\n",
      "New set of weights found, iteration :  243 loss 0.6261096725947449 acc: 0.8833333333333333\n",
      "New set of weights found, iteration :  244 loss 0.6224111115596048 acc: 0.8566666666666667\n",
      "New set of weights found, iteration :  245 loss 0.6148152654807714 acc: 0.8266666666666667\n",
      "New set of weights found, iteration :  247 loss 0.5974671193247704 acc: 0.8433333333333334\n",
      "New set of weights found, iteration :  250 loss 0.583417904283829 acc: 0.8733333333333333\n",
      "New set of weights found, iteration :  257 loss 0.5772679554875276 acc: 0.8866666666666667\n",
      "New set of weights found, iteration :  261 loss 0.5757147587570259 acc: 0.92\n",
      "New set of weights found, iteration :  265 loss 0.5729125391380412 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  269 loss 0.5716186839352565 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  271 loss 0.5624112916282478 acc: 0.92\n",
      "New set of weights found, iteration :  272 loss 0.5614005122726488 acc: 0.89\n",
      "New set of weights found, iteration :  278 loss 0.5464783177415465 acc: 0.86\n",
      "New set of weights found, iteration :  281 loss 0.5456084698093419 acc: 0.8466666666666667\n",
      "New set of weights found, iteration :  283 loss 0.5415786983605987 acc: 0.8666666666666667\n",
      "New set of weights found, iteration :  286 loss 0.5379635618201547 acc: 0.8933333333333333\n",
      "New set of weights found, iteration :  288 loss 0.5360930533849497 acc: 0.8966666666666666\n",
      "New set of weights found, iteration :  290 loss 0.5280937558175479 acc: 0.8533333333333334\n",
      "New set of weights found, iteration :  298 loss 0.524522008872474 acc: 0.82\n",
      "New set of weights found, iteration :  299 loss 0.5135781976198377 acc: 0.8466666666666667\n",
      "New set of weights found, iteration :  301 loss 0.508561054683001 acc: 0.9033333333333333\n",
      "New set of weights found, iteration :  304 loss 0.5015782101710784 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  305 loss 0.4970844434205354 acc: 0.91\n",
      "New set of weights found, iteration :  307 loss 0.49369588663212266 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  308 loss 0.4935766256294617 acc: 0.91\n",
      "New set of weights found, iteration :  312 loss 0.49241375809319 acc: 0.9\n",
      "New set of weights found, iteration :  317 loss 0.48634504405658363 acc: 0.8966666666666666\n",
      "New set of weights found, iteration :  320 loss 0.47602846606578963 acc: 0.9133333333333333\n",
      "New set of weights found, iteration :  329 loss 0.474404841769788 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  333 loss 0.46783023919337136 acc: 0.91\n",
      "New set of weights found, iteration :  334 loss 0.46243361537871785 acc: 0.9066666666666666\n",
      "New set of weights found, iteration :  340 loss 0.4505496389826783 acc: 0.9133333333333333\n",
      "New set of weights found, iteration :  344 loss 0.4482325038716005 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  353 loss 0.4447017121633428 acc: 0.93\n",
      "New set of weights found, iteration :  364 loss 0.44422003590884734 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  366 loss 0.44178288776204644 acc: 0.93\n",
      "New set of weights found, iteration :  369 loss 0.44162371813094237 acc: 0.94\n",
      "New set of weights found, iteration :  373 loss 0.4372247181720769 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  378 loss 0.4353612196828167 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  383 loss 0.43495788072457875 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  385 loss 0.4261062781992333 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  388 loss 0.4245506846589064 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  394 loss 0.4190677215808499 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  396 loss 0.41166456529235385 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  397 loss 0.40923059935688827 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  400 loss 0.40828663798772086 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  401 loss 0.40268963460227963 acc: 0.9233333333333333\n",
      "New set of weights found, iteration :  403 loss 0.3995254914680706 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  414 loss 0.39476678177353325 acc: 0.93\n",
      "New set of weights found, iteration :  415 loss 0.39375848292192445 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  425 loss 0.3889224534257383 acc: 0.9233333333333333\n",
      "New set of weights found, iteration :  427 loss 0.38696850107180947 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  435 loss 0.38021611303448904 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  442 loss 0.3729316290592898 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  445 loss 0.3707425825920926 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  450 loss 0.36012213700221823 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  452 loss 0.3551116025051117 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  453 loss 0.3538668652117077 acc: 0.95\n",
      "New set of weights found, iteration :  459 loss 0.3503723521866273 acc: 0.93\n",
      "New set of weights found, iteration :  462 loss 0.348218088177042 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  463 loss 0.34543638064216714 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  464 loss 0.33975251982175575 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  467 loss 0.3381306404212837 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  468 loss 0.3360121825201582 acc: 0.9133333333333333\n",
      "New set of weights found, iteration :  471 loss 0.32251399963866056 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  472 loss 0.31464708269688557 acc: 0.93\n",
      "New set of weights found, iteration :  479 loss 0.30631123818960365 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  481 loss 0.2982889039618139 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  482 loss 0.2977743791296028 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  483 loss 0.29738287947000036 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  485 loss 0.29240199078587564 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  494 loss 0.29139107253337926 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  497 loss 0.289660061387312 acc: 0.94\n",
      "New set of weights found, iteration :  510 loss 0.2877591836402885 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  515 loss 0.28106994197675267 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  518 loss 0.2806297907625088 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  524 loss 0.2776479288469425 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  525 loss 0.276729806675449 acc: 0.94\n",
      "New set of weights found, iteration :  528 loss 0.27504331844339147 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  532 loss 0.27358741268170833 acc: 0.92\n",
      "New set of weights found, iteration :  535 loss 0.27085114563181606 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  540 loss 0.26771480303292655 acc: 0.94\n",
      "New set of weights found, iteration :  541 loss 0.2668529505048504 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  549 loss 0.26649295384114347 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  553 loss 0.26552346577131647 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  554 loss 0.2598860907733563 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  556 loss 0.25846622765887994 acc: 0.93\n",
      "New set of weights found, iteration :  561 loss 0.25815421626038976 acc: 0.94\n",
      "New set of weights found, iteration :  567 loss 0.25799930709507674 acc: 0.92\n",
      "New set of weights found, iteration :  568 loss 0.2577154278323563 acc: 0.92\n",
      "New set of weights found, iteration :  572 loss 0.2568525773143143 acc: 0.9233333333333333\n",
      "New set of weights found, iteration :  573 loss 0.2543960726805822 acc: 0.92\n",
      "New set of weights found, iteration :  574 loss 0.25325431627404205 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  578 loss 0.2530020196463155 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  579 loss 0.2509549196457911 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  583 loss 0.2508707843267894 acc: 0.9133333333333333\n",
      "New set of weights found, iteration :  586 loss 0.24868562347695947 acc: 0.92\n",
      "New set of weights found, iteration :  588 loss 0.24601427593161546 acc: 0.9133333333333333\n",
      "New set of weights found, iteration :  596 loss 0.24529403238268258 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  599 loss 0.24427123637837894 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  605 loss 0.24421706883060415 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  611 loss 0.24299320861598966 acc: 0.94\n",
      "New set of weights found, iteration :  617 loss 0.24046682909547049 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  620 loss 0.23935652992964188 acc: 0.92\n",
      "New set of weights found, iteration :  624 loss 0.23465428018690015 acc: 0.93\n",
      "New set of weights found, iteration :  627 loss 0.23395193102409706 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  630 loss 0.23277619042619563 acc: 0.9166666666666666\n",
      "New set of weights found, iteration :  631 loss 0.22886926384225134 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  634 loss 0.22820464121573128 acc: 0.94\n",
      "New set of weights found, iteration :  641 loss 0.2268365717432909 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  642 loss 0.2218302352120361 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  643 loss 0.2196650027974537 acc: 0.94\n",
      "New set of weights found, iteration :  644 loss 0.21574229311252982 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  649 loss 0.2136075857514102 acc: 0.94\n",
      "New set of weights found, iteration :  651 loss 0.20888620000700267 acc: 0.93\n",
      "New set of weights found, iteration :  655 loss 0.20824102808837344 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  660 loss 0.20801927954190227 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  662 loss 0.20785637301465443 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  664 loss 0.2068252540157598 acc: 0.93\n",
      "New set of weights found, iteration :  683 loss 0.205678052311531 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  694 loss 0.20457075788249024 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  711 loss 0.20255068287038827 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  723 loss 0.20206183464865546 acc: 0.93\n",
      "New set of weights found, iteration :  735 loss 0.200637887727164 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  743 loss 0.20052475532286132 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  745 loss 0.19787117024138798 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  747 loss 0.19761241135299482 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  750 loss 0.19572567883291275 acc: 0.9233333333333333\n",
      "New set of weights found, iteration :  764 loss 0.19488079229655209 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  773 loss 0.19436215910332091 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  775 loss 0.19411262043923727 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  788 loss 0.19364759931257824 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  814 loss 0.1926934804606148 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  867 loss 0.1922868738616263 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  875 loss 0.19217141158455306 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  876 loss 0.19070596457292935 acc: 0.9233333333333333\n",
      "New set of weights found, iteration :  885 loss 0.1902880323924425 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  891 loss 0.19017756693369883 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  893 loss 0.18946783196383754 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  894 loss 0.1877666381126891 acc: 0.93\n",
      "New set of weights found, iteration :  906 loss 0.187165422030565 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  911 loss 0.1867367540277796 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  918 loss 0.18668491645284457 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  920 loss 0.18640640491977786 acc: 0.94\n",
      "New set of weights found, iteration :  933 loss 0.185999690173266 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  936 loss 0.18591101307027885 acc: 0.94\n",
      "New set of weights found, iteration :  938 loss 0.18561362640887905 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  940 loss 0.1844910902668657 acc: 0.93\n",
      "New set of weights found, iteration :  946 loss 0.18407070175151527 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  965 loss 0.1840514508098802 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  967 loss 0.18332674430437496 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  991 loss 0.18285590762405846 acc: 0.9266666666666666\n",
      "New set of weights found, iteration :  992 loss 0.18272180539393948 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1001 loss 0.18252465134187987 acc: 0.9466666666666667\n",
      "New set of weights found, iteration :  1025 loss 0.18159171008188338 acc: 0.93\n",
      "New set of weights found, iteration :  1034 loss 0.18004769944809812 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1061 loss 0.1797334993089863 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  1072 loss 0.17944704846937048 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  1074 loss 0.1790560616806018 acc: 0.94\n",
      "New set of weights found, iteration :  1077 loss 0.17902707045843902 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  1085 loss 0.17891380727875072 acc: 0.93\n",
      "New set of weights found, iteration :  1101 loss 0.17830006863765152 acc: 0.93\n",
      "New set of weights found, iteration :  1102 loss 0.17805822717542938 acc: 0.93\n",
      "New set of weights found, iteration :  1104 loss 0.17750779309816553 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  1140 loss 0.1772097541361821 acc: 0.93\n",
      "New set of weights found, iteration :  1143 loss 0.17707585517566093 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1144 loss 0.1764799697234672 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1152 loss 0.17607729734625133 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1280 loss 0.17602010250370045 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  1311 loss 0.1757959472564639 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  1345 loss 0.17557927185754274 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1417 loss 0.17554842151461802 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1470 loss 0.1754171668680365 acc: 0.94\n",
      "New set of weights found, iteration :  1482 loss 0.17536440374937523 acc: 0.9433333333333334\n",
      "New set of weights found, iteration :  1527 loss 0.17527103741317968 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1533 loss 0.17503236659245824 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  1545 loss 0.17498080816512435 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  1620 loss 0.17472896614506264 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  1845 loss 0.17459968348873914 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  2374 loss 0.17448433994892384 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  2381 loss 0.1744415517443196 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  2630 loss 0.17442720226528058 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  2760 loss 0.17430381563240174 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  2874 loss 0.17428789664220223 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  3359 loss 0.1742675399113225 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  3532 loss 0.1741157489168961 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  8311 loss 0.17409500935252878 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  8976 loss 0.17401644314438167 acc: 0.9366666666666666\n",
      "New set of weights found, iteration :  9038 loss 0.17399700561444104 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  9125 loss 0.17392679373678072 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  9146 loss 0.1739212203588457 acc: 0.9333333333333333\n",
      "New set of weights found, iteration :  9786 loss 0.17381628271738156 acc: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Full code regarding using the loss function to tweak the weights :\n",
    "\n",
    "X, y = create_data(samples=100,classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "lowest_loss = 999999\n",
    "\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = best_dense1_biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(10000):\n",
    "    \n",
    "    dense1.weights += 0.05 * np.random.randn(2,3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1,3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3,3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1,3)\n",
    "    \n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration : ' , iteration, 'loss', loss, 'acc:',accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter 7 Derivative\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[0 2 4 6 8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n",
    "x = np.array(range(5))\n",
    "y =  f(x)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x**2\n",
    "y= f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 2*x**2\n",
    "\n",
    "x= np.arange(0,5,0.001)\n",
    "y = f(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8) (2.0001, 8.000800020000002)\n",
      "Approximate derivative for f(x) where x= 2 is 8.000199999998785\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def f(x):\n",
    "    return 2*x**2\n",
    "\n",
    "x= np.arange(0,5,0.001)\n",
    "y=f(x)\n",
    "plt.plot(x,y)\n",
    "p2_delta = 0.0001\n",
    "x1= 2\n",
    "x2 = x1 +p2_delta\n",
    "y1 = f(x1)\n",
    "y2 = f(x2)\n",
    "\n",
    "print((x1,y1),(x2,y2))\n",
    "\n",
    "approximate_derivative = (y2-y1)/(x2-x1)\n",
    "b = y2 - approximate_derivative*x2\n",
    "\n",
    "def tangent_line(x):\n",
    "    return approximate_derivative*x + b\n",
    "\n",
    "to_plot = [x1-0.9, x1, x1+0.9]\n",
    "plt.plot(to_plot,[tangent_line(i) for i in to_plot])\n",
    "\n",
    "print('Approximate derivative for f(x)', f'where x= {x1} is {approximate_derivative}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "#Backpropagation\n",
    "x = [1.0,-2.0,3.0]\n",
    "w = [-3.0,-1.0,2.0]\n",
    "b = 1.0\n",
    "\n",
    "xw0= x[0] * w[0]\n",
    "xw1= x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "#ReLU activation function : \n",
    "y = max(z,0)\n",
    "\n",
    "#ReLU display z if positive\n",
    "\n",
    "#The derivative from the next layer manually set\n",
    "dvalue = 1.0\n",
    "#Derivation of ReLU and the chain rule :\n",
    "#Recall that the derivative of ReLU() with respect to its input is 1, if the input is greater than 0 and 0 otherwise.\n",
    "drelu_dz = (1. if z > 0 else 0.)\n",
    "print(drelu_dz)\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1.,1.,1.]])\n",
    "\n",
    "weights = np.array([[0.2,0.8,-0.5,1],\n",
    "                    [0.5,-0.91,0.26,-0.5],\n",
    "                    [-0.26,-0.27,0.17,0.87]\n",
    "                    ]).T\n",
    "\n",
    "#sum weights of given input\n",
    "#and mulitply by the passed in gradient for this neuron\n",
    "dx0 = sum(weights[0]*dvalues[0])\n",
    "dx1 = sum(weights[1]*dvalues[0])\n",
    "dx2 = sum(weights[2]*dvalues[0])\n",
    "dx3 = sum(weights[3]*dvalues[0])\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dvalues = np.array([[1., 1., 1.]])\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "[0.5, -0.91, 0.26, -0.5],\n",
    "[-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot() missing 2 required positional arguments: 'a' and 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m dweights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(inputs\u001b[38;5;241m.\u001b[39mT,dvalues)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(dvalues)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(dweights)\n",
      "\u001b[1;31mTypeError\u001b[0m: dot() missing 2 required positional arguments: 'a' and 'b'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.]])\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "[2., 5., -1., 2],\n",
    "[-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dweights = np.dot(inputs.T,dvalues)\n",
    "\n",
    "print(dvalues)\n",
    "np.dot()\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x = [1,-2,3] #inputs value\n",
    "w = [-3,-1,2] #Weight value\n",
    "b = 1 #bias value\n",
    "\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "#We then perform a sum :\n",
    "z = xw0 + xw2 + xw1 + b\n",
    "\n",
    "#Then we pass the sum to the ReLU activation function \n",
    "y = max(z,0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.001 1 -0.998 -2 1.997 3\n",
      "[-3.002, -0.996, 1.9940000000000002] 0.998\n"
     ]
    }
   ],
   "source": [
    "#A COMPRENDRE ABSOLUMENT POUR CONTINUER\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Derivative of the ReLU with respect to x0 is 1 if the sum (in this case its input) is greater than 0 then the derivative is 1\n",
    "\n",
    "drelu_dz = (1 if z > 0 else 0)\n",
    "\n",
    "#The derivatie from the next layer because we assume that during the previous backpropagation of the next layer we received the value of one\n",
    "dvalue = 1\n",
    "\n",
    "#This is the chain rule\n",
    "drelu_dz = dvalue * drelu_dz\n",
    "\n",
    "#Dans ce cas spécifique nous cherchons l'impact de w0 sur la fonction mais il faut pour chaque wreight calculer la dériver de la somme par rapport a x1 ,x2 ...\n",
    "dsum_dxw0 = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0 #Dérivée quand on cherche l'impact de w0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1 #Derivée quand on cherche l'impact de w1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2#Dérivée quand on cherche l'impact de w2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "\n",
    "\n",
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # gradients on weights\n",
    "db = drelu_db # gradient on bias...just 1 bias here.\n",
    "\n",
    "\n",
    "#We will get more in depth with the optimizer chapter but still that is working correctly\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "print(w, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9700000000000015\n"
     ]
    }
   ],
   "source": [
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Passed in gradient from the next layer\n",
    "#for the purpose of this example we re going to use a vector of 1s\n",
    "\n",
    "dvalues = np.array([[1.,1.,1.]])\n",
    "\n",
    "#We have 3 sets of weights - one set for each neuron\n",
    "#we have 4 inputs, this 4 weights\n",
    "# recall that we keep weights transposed\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "[0.5, -0.91, 0.26, -0.5],\n",
    "[-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed in gradient for this neuron\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.]])\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "[2., 5., -1., 2],\n",
    "[-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "\n",
    "dweights = np.dot(inputs.T,dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Passed in batches of gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.]])\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "[2, -7, -1, 3],\n",
    "[-1, 2, 5, -1]])\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "[5, 6, 7, 8],\n",
    "[9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activation's derivative\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z > 0] = 1\n",
    "\n",
    "print(drelu)\n",
    "\n",
    "drelu *= dvalues\n",
    "\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "[2, -7, -1, 3],\n",
    "[-1, 2, 5, -1]])\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "[5, 6, 7, 8],\n",
    "[9, 10, 11, 12]])\n",
    "\n",
    "drelu = dvalues.copy()\n",
    "\n",
    "drelu[z <= 0] = 0\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Passed in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.]])\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "[2., 5., -1., 2],\n",
    "[-1.5, 2.7, 3.3, -0.8]])\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "[0.5, -0.91, 0.26, -0.5],\n",
    "[-0.26, -0.27, 0.17, 0.87]]).T\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "\n",
    "#Forward Pass\n",
    "layer_outputs = np.dot(inputs,weights) + biases\n",
    "relu_outputs = np.maximum(0,layer_outputs)\n",
    "\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "#Dense Layer\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "\n",
    "dbiases = np.sum(drelu, axis=0,keepdims=True)\n",
    "\n",
    "#Update the parameters (we will see this in details during the Optimizer chapter)\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# Zero gradient where input values were negative\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLoss_CategoricalCrossentropy\u001b[39;00m(\u001b[43mLoss\u001b[49m):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,y_pred,y_true):\n\u001b[0;32m     32\u001b[0m         samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Loss' is not defined"
     ]
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights = 0.01 * np.random.randn(inputs, neurons)\n",
    "        self.biases = np.zeros((1,neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    \n",
    "    def forward(self,y_pred,y_true):\n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        \n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "            \n",
    "        elif len(y_true.shape) == 2 :\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,axis=1\n",
    "            )\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        \n",
    "        return negative_log_likelihoods  \n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        self.dinputs = -y_true /dvalues\n",
    "        self.dinputs = self.dinputs / samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7]\n",
      " [0.1]\n",
      " [0.2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "softmax_output = [0.7,0.1,0.2]\n",
    "softmax_output = np.array(softmax_output).reshape(-1,1)\n",
    "\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probabilities\n",
    "    \n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs =  np.empty_like(dvalues)\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self,inputs,y_true):\n",
    "        \n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "        \n",
    "    def backward(self,dvalues,y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "            \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write both solutions\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__ (self,n_inputs,n_neurons):\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "              \n",
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward (self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n",
    "class Loss:\n",
    "    def calculate (self,output,y):\n",
    "        \n",
    "        sample_losses = self.forward(output,y)\n",
    "        \n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "\n",
    "       \n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "        keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "        keepdims=True)\n",
    "        self.output = probabilities\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "        enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "            np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "            single_dvalues)\n",
    "            \n",
    "            \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred,y_true):\n",
    "        \n",
    "        \n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "            self.dinputs = -y_true / dvalues\n",
    "            self.dinputs = self.dinputs / samples\n",
    "            \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "    # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients : combined loss and activation : \n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients : separate loss and activation : \n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "[0.1, 0.5, 0.4],\n",
    "[0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([0, 1, 1])\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "\n",
    "print('Gradients : combined loss and activation : ')\n",
    "print(dvalues1)\n",
    "print('Gradients : separate loss and activation : ')\n",
    "print(dvalues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Copyright (c) 2015 Andrej Karpathy\n",
    "# License: https://github.com/cs231n/cs231n.github.io/blob/master/LICENSE\n",
    "# Source: https://cs231n.github.io/neural-networks-case-study/\n",
    "def create_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.0986113541054958\n",
      "acc 0.37666666666666665\n",
      "[[-0.12155149  0.15928704  0.05544897]\n",
      " [-0.19058199 -0.0212509   0.08131245]]\n",
      "[[0.40590301 0.32186021 0.18421025]]\n",
      "[[-6.59737342e-05  1.62199501e-04 -9.62257663e-05]\n",
      " [-2.84881178e-05  3.98756023e-05 -1.13874845e-05]\n",
      " [-9.59556398e-06 -3.98188375e-05  4.94144015e-05]]\n",
      "[[ 9.05458533e-06 -4.54865730e-07 -8.59971960e-06]]\n"
     ]
    }
   ],
   "source": [
    "X, y = create_data(samples=100, classes=3)\n",
    "\n",
    "#On créer un layer avec 2 inputs et 3 neurons\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "#On défini la fonction d'activation qui servira pour notre layer1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#On défini un deuxieme layer avec les 3 outputs précédent et 3 nouveaux neurons\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "#On défini la fonction d'activation du deuxieme layer ici la softmax (si la fonction semnble étrange c'est que pour des raisons d'optimization le traitement de la backpropagation de la loss est associé avec l'activation)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "#On fais passer par batch les inputs dans le premier layer avec les weigths initialisé aléatoirement et les bias = 0\n",
    "dense1.forward(X)\n",
    "\n",
    "#On fais passer les inputs * weights + bias dans notre fonction d'activation \n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "#On recupere les outputs du forward passer dans notre function d'activation ReLU et on les re mutiplie par les weights * inputs + bias\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "#Ici on devrais juste passer les outputs a traver la fonction d'activation mais pour des raison d'optimisations on les faits passer dans la fonction d'activation et on calcul la pertes en comparant avec les prédiction contenues dans la variable y:\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y )\n",
    "\n",
    "print(\"loss\" , loss)\n",
    "\n",
    "#Calculate accuracy from output of activation2 and targets \n",
    "#Calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y,axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print('acc', accuracy)\n",
    "\n",
    "#Ici on recuperes les gradients de la fonction de pertes, ainsi que la fonction d'activation\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "\n",
    "#On fais passer les gradients et on les recuperes pour les multiplier grace à la chain rule aux gradients de la somme et de la multiplications, On va avoir normallement deux types de gradients ici : le gradients des inputs obtenues par les dérivées partielles en fonctions de inputs et le gradient des weights obtenue par la dérivation en fonction des weights \n",
    "dense2.backward(loss_activation.dinputs)\n",
    "\n",
    "#On fais passer les gradients par respect aux inputs dans la prochaine fonctions d'activations (cela revient a faire une multiplication):\n",
    "activation1.backward(dense2.dinputs)\n",
    "\n",
    "#On continue d'appliquer la chain rule avec cette fois ci le passages des gradients * les dérivé de la function ReLU\n",
    "activation1.backward(dense2.inputs)\n",
    "\n",
    "#Enfin on comple avec la derive de la somme et des différentes multiplication\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get to the Chapter 10 Optimizer : \n",
    "\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    def __init__(self,learning_rate=.85):\n",
    "        self.learning_rate = learning_rate\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "        \n",
    "optimizer = Optimizer_SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.0986484151189055\n",
      "acc 0.3233333333333333\n"
     ]
    }
   ],
   "source": [
    "X, y = create_data(samples=100, classes=3)\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64,3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "#Create optimizer\n",
    "\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "dense1.forward(X)\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "\n",
    "print('loss', loss)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y,axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print('acc', accuracy)\n",
    "\n",
    "\n",
    "#Then we do our backwardpass : \n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 acc : 0.343 loss : 1.099\n",
      "epoch : 100 acc : 0.423 loss : 1.082\n",
      "epoch : 200 acc : 0.433 loss : 1.071\n",
      "epoch : 300 acc : 0.447 loss : 1.068\n",
      "epoch : 400 acc : 0.463 loss : 1.065\n",
      "epoch : 500 acc : 0.473 loss : 1.062\n",
      "epoch : 600 acc : 0.467 loss : 1.059\n",
      "epoch : 700 acc : 0.473 loss : 1.053\n",
      "epoch : 800 acc : 0.497 loss : 1.042\n",
      "epoch : 900 acc : 0.537 loss : 1.027\n",
      "epoch : 1000 acc : 0.473 loss : 1.023\n",
      "epoch : 1100 acc : 0.493 loss : 1.011\n",
      "epoch : 1200 acc : 0.507 loss : 1.002\n",
      "epoch : 1300 acc : 0.490 loss : 0.988\n",
      "epoch : 1400 acc : 0.540 loss : 0.998\n",
      "epoch : 1500 acc : 0.420 loss : 1.000\n",
      "epoch : 1600 acc : 0.477 loss : 0.979\n",
      "epoch : 1700 acc : 0.490 loss : 0.960\n",
      "epoch : 1800 acc : 0.527 loss : 0.963\n",
      "epoch : 1900 acc : 0.503 loss : 0.954\n",
      "epoch : 2000 acc : 0.497 loss : 0.938\n",
      "epoch : 2100 acc : 0.550 loss : 0.927\n",
      "epoch : 2200 acc : 0.597 loss : 0.911\n",
      "epoch : 2300 acc : 0.550 loss : 0.892\n",
      "epoch : 2400 acc : 0.600 loss : 0.866\n",
      "epoch : 2500 acc : 0.527 loss : 0.902\n",
      "epoch : 2600 acc : 0.607 loss : 0.848\n",
      "epoch : 2700 acc : 0.617 loss : 0.800\n",
      "epoch : 2800 acc : 0.533 loss : 0.922\n",
      "epoch : 2900 acc : 0.610 loss : 0.879\n",
      "epoch : 3000 acc : 0.520 loss : 1.070\n",
      "epoch : 3100 acc : 0.607 loss : 0.824\n",
      "epoch : 3200 acc : 0.663 loss : 0.731\n",
      "epoch : 3300 acc : 0.667 loss : 0.711\n",
      "epoch : 3400 acc : 0.637 loss : 0.731\n",
      "epoch : 3500 acc : 0.670 loss : 0.722\n",
      "epoch : 3600 acc : 0.663 loss : 0.714\n",
      "epoch : 3700 acc : 0.647 loss : 0.706\n",
      "epoch : 3800 acc : 0.660 loss : 0.728\n",
      "epoch : 3900 acc : 0.690 loss : 0.679\n",
      "epoch : 4000 acc : 0.703 loss : 0.659\n",
      "epoch : 4100 acc : 0.493 loss : 1.309\n",
      "epoch : 4200 acc : 0.683 loss : 0.675\n",
      "epoch : 4300 acc : 0.707 loss : 0.648\n",
      "epoch : 4400 acc : 0.713 loss : 0.634\n",
      "epoch : 4500 acc : 0.700 loss : 0.650\n",
      "epoch : 4600 acc : 0.693 loss : 0.641\n",
      "epoch : 4700 acc : 0.590 loss : 1.044\n",
      "epoch : 4800 acc : 0.633 loss : 0.805\n",
      "epoch : 4900 acc : 0.747 loss : 0.578\n",
      "epoch : 5000 acc : 0.730 loss : 0.598\n",
      "epoch : 5100 acc : 0.720 loss : 0.630\n",
      "epoch : 5200 acc : 0.747 loss : 0.559\n",
      "epoch : 5300 acc : 0.697 loss : 0.646\n",
      "epoch : 5400 acc : 0.710 loss : 0.633\n",
      "epoch : 5500 acc : 0.717 loss : 0.602\n",
      "epoch : 5600 acc : 0.730 loss : 0.628\n",
      "epoch : 5700 acc : 0.750 loss : 0.584\n",
      "epoch : 5800 acc : 0.740 loss : 0.609\n",
      "epoch : 5900 acc : 0.730 loss : 0.598\n",
      "epoch : 6000 acc : 0.730 loss : 0.589\n",
      "epoch : 6100 acc : 0.670 loss : 0.772\n",
      "epoch : 6200 acc : 0.760 loss : 0.540\n",
      "epoch : 6300 acc : 0.753 loss : 0.562\n",
      "epoch : 6400 acc : 0.740 loss : 0.606\n",
      "epoch : 6500 acc : 0.760 loss : 0.588\n",
      "epoch : 6600 acc : 0.767 loss : 0.581\n",
      "epoch : 6700 acc : 0.770 loss : 0.578\n",
      "epoch : 6800 acc : 0.753 loss : 0.578\n",
      "epoch : 6900 acc : 0.767 loss : 0.571\n",
      "epoch : 7000 acc : 0.730 loss : 0.589\n",
      "epoch : 7100 acc : 0.767 loss : 0.576\n",
      "epoch : 7200 acc : 0.767 loss : 0.567\n",
      "epoch : 7300 acc : 0.740 loss : 0.555\n",
      "epoch : 7400 acc : 0.810 loss : 0.485\n",
      "epoch : 7500 acc : 0.757 loss : 0.560\n",
      "epoch : 7600 acc : 0.757 loss : 0.550\n",
      "epoch : 7700 acc : 0.753 loss : 0.536\n",
      "epoch : 7800 acc : 0.753 loss : 0.529\n",
      "epoch : 7900 acc : 0.757 loss : 0.523\n",
      "epoch : 8000 acc : 0.777 loss : 0.536\n",
      "epoch : 8100 acc : 0.780 loss : 0.526\n",
      "epoch : 8200 acc : 0.780 loss : 0.524\n",
      "epoch : 8300 acc : 0.780 loss : 0.522\n",
      "epoch : 8400 acc : 0.773 loss : 0.542\n",
      "epoch : 8500 acc : 0.790 loss : 0.495\n",
      "epoch : 8600 acc : 0.760 loss : 0.491\n",
      "epoch : 8700 acc : 0.767 loss : 0.493\n",
      "epoch : 8800 acc : 0.763 loss : 0.501\n",
      "epoch : 8900 acc : 0.767 loss : 0.505\n",
      "epoch : 9000 acc : 0.767 loss : 0.498\n",
      "epoch : 9100 acc : 0.767 loss : 0.483\n",
      "epoch : 9200 acc : 0.763 loss : 0.484\n",
      "epoch : 9300 acc : 0.763 loss : 0.486\n",
      "epoch : 9400 acc : 0.767 loss : 0.490\n",
      "epoch : 9500 acc : 0.757 loss : 0.511\n",
      "epoch : 9600 acc : 0.770 loss : 0.493\n",
      "epoch : 9700 acc : 0.773 loss : 0.471\n",
      "epoch : 9800 acc : 0.763 loss : 0.476\n",
      "epoch : 9900 acc : 0.760 loss : 0.533\n",
      "epoch : 10000 acc : 0.803 loss : 0.472\n"
     ]
    }
   ],
   "source": [
    "#We now do it performing multiple epoch to get to a more precise weights and biases optimization\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y )\n",
    "    predictions = np.argmax(loss_activation.output,axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y,axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch : {epoch}',\n",
    "              f'acc : {accuracy:.3f}',\n",
    "              f'loss : {loss:.3f}')\n",
    "    \n",
    "    #Backward pass\n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, y )\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    \n",
    "    #Update weights and biases :\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "#To not be blocked on the local minimum we need a strategy involving the learning rate decay : So that the steps are not too small or too big \n",
    "starting_learning_rate = 1\n",
    "learning_rate_decay = 0.1\n",
    "\n",
    "step = 1\n",
    "\n",
    "learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "\n",
    "print(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9090909090909091\n",
      "0.8333333333333334\n",
      "0.7692307692307692\n",
      "0.7142857142857143\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.588235294117647\n",
      "0.5555555555555556\n",
      "0.5263157894736842\n",
      "0.5\n",
      "0.47619047619047616\n",
      "0.45454545454545453\n",
      "0.4347826086956522\n",
      "0.41666666666666663\n",
      "0.4\n",
      "0.3846153846153846\n",
      "0.37037037037037035\n",
      "0.35714285714285715\n",
      "0.3448275862068965\n"
     ]
    }
   ],
   "source": [
    "#The previous code was a way to give an example at a precise step but we will implement it in a loop : \n",
    "\n",
    "starting_learning_rate = 1\n",
    "learning_rate_decay = 0.1\n",
    "\n",
    "for step in range(20):\n",
    "    learning_rate = starting_learning_rate * (1 / (1  + learning_rate_decay * step))\n",
    "    print(learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD: \n",
    "    \n",
    "    def __init__(self,learning_rate=1,decay=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 , acc : 0.3233333333333333 , loss : 1.099,lr:1\n",
      "epoch : 100 , acc : 0.3233333333333333 , loss : 1.068,lr:0.9099181073703367\n",
      "epoch : 200 , acc : 0.3233333333333333 , loss : 1.056,lr:0.8340283569641367\n",
      "epoch : 300 , acc : 0.3233333333333333 , loss : 1.055,lr:0.7698229407236336\n",
      "epoch : 400 , acc : 0.3233333333333333 , loss : 1.054,lr:0.7147962830593281\n",
      "epoch : 500 , acc : 0.3233333333333333 , loss : 1.053,lr:0.66711140760507\n",
      "epoch : 600 , acc : 0.3233333333333333 , loss : 1.052,lr:0.6253908692933083\n",
      "epoch : 700 , acc : 0.3233333333333333 , loss : 1.051,lr:0.5885815185403178\n",
      "epoch : 800 , acc : 0.3233333333333333 , loss : 1.050,lr:0.5558643690939411\n",
      "epoch : 900 , acc : 0.3233333333333333 , loss : 1.049,lr:0.526592943654555\n",
      "epoch : 1000 , acc : 0.3233333333333333 , loss : 1.048,lr:0.5002501250625312\n",
      "epoch : 1100 , acc : 0.3233333333333333 , loss : 1.047,lr:0.4764173415912339\n",
      "epoch : 1200 , acc : 0.3233333333333333 , loss : 1.047,lr:0.45475216007276037\n",
      "epoch : 1300 , acc : 0.3233333333333333 , loss : 1.046,lr:0.43497172683775553\n",
      "epoch : 1400 , acc : 0.3233333333333333 , loss : 1.046,lr:0.4168403501458941\n",
      "epoch : 1500 , acc : 0.3233333333333333 , loss : 1.045,lr:0.4001600640256102\n",
      "epoch : 1600 , acc : 0.3233333333333333 , loss : 1.045,lr:0.3847633705271258\n",
      "epoch : 1700 , acc : 0.3233333333333333 , loss : 1.044,lr:0.3705075954057058\n",
      "epoch : 1800 , acc : 0.3233333333333333 , loss : 1.043,lr:0.35727045373347627\n",
      "epoch : 1900 , acc : 0.3233333333333333 , loss : 1.042,lr:0.3449465332873405\n",
      "epoch : 2000 , acc : 0.3233333333333333 , loss : 1.041,lr:0.33344448149383127\n",
      "epoch : 2100 , acc : 0.3233333333333333 , loss : 1.040,lr:0.32268473701193934\n",
      "epoch : 2200 , acc : 0.3233333333333333 , loss : 1.038,lr:0.31259768677711786\n",
      "epoch : 2300 , acc : 0.3233333333333333 , loss : 1.036,lr:0.3031221582297666\n",
      "epoch : 2400 , acc : 0.3233333333333333 , loss : 1.033,lr:0.29420417769932333\n",
      "epoch : 2500 , acc : 0.3233333333333333 , loss : 1.030,lr:0.2857959416976279\n",
      "epoch : 2600 , acc : 0.3233333333333333 , loss : 1.027,lr:0.2778549597110308\n",
      "epoch : 2700 , acc : 0.3233333333333333 , loss : 1.024,lr:0.2703433360367667\n",
      "epoch : 2800 , acc : 0.3233333333333333 , loss : 1.021,lr:0.26322716504343247\n",
      "epoch : 2900 , acc : 0.3233333333333333 , loss : 1.017,lr:0.25647601949217746\n",
      "epoch : 3000 , acc : 0.3233333333333333 , loss : 1.013,lr:0.25006251562890724\n",
      "epoch : 3100 , acc : 0.3233333333333333 , loss : 1.010,lr:0.2439619419370578\n",
      "epoch : 3200 , acc : 0.3233333333333333 , loss : 1.006,lr:0.23815194093831865\n",
      "epoch : 3300 , acc : 0.3233333333333333 , loss : 1.003,lr:0.23261223540358225\n",
      "epoch : 3400 , acc : 0.3233333333333333 , loss : 0.999,lr:0.22732439190725165\n",
      "epoch : 3500 , acc : 0.3233333333333333 , loss : 0.996,lr:0.22227161591464767\n",
      "epoch : 3600 , acc : 0.3233333333333333 , loss : 0.993,lr:0.21743857360295715\n",
      "epoch : 3700 , acc : 0.3233333333333333 , loss : 0.990,lr:0.21281123643328367\n",
      "epoch : 3800 , acc : 0.3233333333333333 , loss : 0.987,lr:0.20837674515524068\n",
      "epoch : 3900 , acc : 0.3233333333333333 , loss : 0.984,lr:0.20412329046744235\n",
      "epoch : 4000 , acc : 0.3233333333333333 , loss : 0.981,lr:0.2000400080016003\n",
      "epoch : 4100 , acc : 0.3233333333333333 , loss : 0.978,lr:0.19611688566385566\n",
      "epoch : 4200 , acc : 0.3233333333333333 , loss : 0.975,lr:0.19234468166955185\n",
      "epoch : 4300 , acc : 0.3233333333333333 , loss : 0.972,lr:0.18871485185884126\n",
      "epoch : 4400 , acc : 0.3233333333333333 , loss : 0.969,lr:0.18521948508983144\n",
      "epoch : 4500 , acc : 0.3233333333333333 , loss : 0.966,lr:0.18185124568103292\n",
      "epoch : 4600 , acc : 0.3233333333333333 , loss : 0.964,lr:0.1786033220217896\n",
      "epoch : 4700 , acc : 0.3233333333333333 , loss : 0.961,lr:0.1754693805930865\n",
      "epoch : 4800 , acc : 0.3233333333333333 , loss : 0.958,lr:0.17244352474564578\n",
      "epoch : 4900 , acc : 0.3233333333333333 , loss : 0.954,lr:0.16952025767079165\n",
      "epoch : 5000 , acc : 0.3233333333333333 , loss : 0.951,lr:0.16669444907484582\n",
      "epoch : 5100 , acc : 0.3233333333333333 , loss : 0.948,lr:0.16396130513198884\n",
      "epoch : 5200 , acc : 0.3233333333333333 , loss : 0.944,lr:0.16131634134537828\n",
      "epoch : 5300 , acc : 0.3233333333333333 , loss : 0.941,lr:0.15875535799333226\n",
      "epoch : 5400 , acc : 0.3233333333333333 , loss : 0.937,lr:0.1562744178777934\n",
      "epoch : 5500 , acc : 0.3233333333333333 , loss : 0.933,lr:0.15386982612709646\n",
      "epoch : 5600 , acc : 0.3233333333333333 , loss : 0.930,lr:0.15153811183512653\n",
      "epoch : 5700 , acc : 0.3233333333333333 , loss : 0.926,lr:0.14927601134497687\n",
      "epoch : 5800 , acc : 0.3233333333333333 , loss : 0.923,lr:0.14708045300779526\n",
      "epoch : 5900 , acc : 0.3233333333333333 , loss : 0.919,lr:0.14494854326714016\n",
      "epoch : 6000 , acc : 0.3233333333333333 , loss : 0.915,lr:0.1428775539362766\n",
      "epoch : 6100 , acc : 0.3233333333333333 , loss : 0.911,lr:0.1408649105507818\n",
      "epoch : 6200 , acc : 0.3233333333333333 , loss : 0.907,lr:0.13890818169190167\n",
      "epoch : 6300 , acc : 0.3233333333333333 , loss : 0.903,lr:0.13700506918755992\n",
      "epoch : 6400 , acc : 0.3233333333333333 , loss : 0.900,lr:0.13515339910798757\n",
      "epoch : 6500 , acc : 0.3233333333333333 , loss : 0.896,lr:0.13335111348179757\n",
      "epoch : 6600 , acc : 0.3233333333333333 , loss : 0.892,lr:0.13159626266614027\n",
      "epoch : 6700 , acc : 0.3233333333333333 , loss : 0.888,lr:0.12988699831146902\n",
      "epoch : 6800 , acc : 0.3233333333333333 , loss : 0.884,lr:0.12822156686754713\n",
      "epoch : 6900 , acc : 0.3233333333333333 , loss : 0.880,lr:0.126598303582732\n",
      "epoch : 7000 , acc : 0.3233333333333333 , loss : 0.876,lr:0.12501562695336915\n",
      "epoch : 7100 , acc : 0.3233333333333333 , loss : 0.873,lr:0.12347203358439313\n",
      "epoch : 7200 , acc : 0.3233333333333333 , loss : 0.869,lr:0.12196609342602757\n",
      "epoch : 7300 , acc : 0.3233333333333333 , loss : 0.866,lr:0.12049644535486204\n",
      "epoch : 7400 , acc : 0.3233333333333333 , loss : 0.862,lr:0.11906179307060363\n",
      "epoch : 7500 , acc : 0.3233333333333333 , loss : 0.858,lr:0.11766090128250381\n",
      "epoch : 7600 , acc : 0.3233333333333333 , loss : 0.855,lr:0.11629259216187929\n",
      "epoch : 7700 , acc : 0.3233333333333333 , loss : 0.851,lr:0.11495574203931487\n",
      "epoch : 7800 , acc : 0.3233333333333333 , loss : 0.848,lr:0.11364927832708263\n",
      "epoch : 7900 , acc : 0.3233333333333333 , loss : 0.844,lr:0.11237217664906168\n",
      "epoch : 8000 , acc : 0.3233333333333333 , loss : 0.841,lr:0.11112345816201799\n",
      "epoch : 8100 , acc : 0.3233333333333333 , loss : 0.838,lr:0.10990218705352237\n",
      "epoch : 8200 , acc : 0.3233333333333333 , loss : 0.834,lr:0.10870746820306555\n",
      "epoch : 8300 , acc : 0.3233333333333333 , loss : 0.830,lr:0.1075384449940854\n",
      "epoch : 8400 , acc : 0.3233333333333333 , loss : 0.827,lr:0.10639429726566654\n",
      "epoch : 8500 , acc : 0.3233333333333333 , loss : 0.824,lr:0.10527423939362038\n",
      "epoch : 8600 , acc : 0.3233333333333333 , loss : 0.821,lr:0.10417751849150952\n",
      "epoch : 8700 , acc : 0.3233333333333333 , loss : 0.819,lr:0.10310341272296113\n",
      "epoch : 8800 , acc : 0.3233333333333333 , loss : 0.816,lr:0.1020512297173181\n",
      "epoch : 8900 , acc : 0.3233333333333333 , loss : 0.813,lr:0.10102030508132134\n",
      "epoch : 9000 , acc : 0.3233333333333333 , loss : 0.811,lr:0.1000100010001\n",
      "epoch : 9100 , acc : 0.3233333333333333 , loss : 0.808,lr:0.09901970492127933\n",
      "epoch : 9200 , acc : 0.3233333333333333 , loss : 0.806,lr:0.09804882831650162\n",
      "epoch : 9300 , acc : 0.3233333333333333 , loss : 0.804,lr:0.09709680551509856\n",
      "epoch : 9400 , acc : 0.3233333333333333 , loss : 0.801,lr:0.09616309260505818\n",
      "epoch : 9500 , acc : 0.3233333333333333 , loss : 0.799,lr:0.09524716639679968\n",
      "epoch : 9600 , acc : 0.3233333333333333 , loss : 0.797,lr:0.09434852344560807\n",
      "epoch : 9700 , acc : 0.3233333333333333 , loss : 0.795,lr:0.09346667912889055\n",
      "epoch : 9800 , acc : 0.3233333333333333 , loss : 0.793,lr:0.09260116677470137\n",
      "epoch : 9900 , acc : 0.3233333333333333 , loss : 0.791,lr:0.09175153683824203\n",
      "epoch : 10000 , acc : 0.3233333333333333 , loss : 0.789,lr:0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = create_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch : {epoch} , ' +\n",
    "              f'acc : {accuracy} , ' +\n",
    "              f'loss : {loss:.3f},' + \n",
    "              f'lr:{optimizer.current_learning_rate}')\n",
    "        \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "# Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. / (1. + self.decay * self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "            self.momentum * layer.weight_momentums - \\\n",
    "            self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "            self.momentum * layer.bias_momentums - \\\n",
    "            self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "# Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "            layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "            layer.dbiases\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X,y \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data\u001b[49m(samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m dense1 \u001b[38;5;241m=\u001b[39m Layer_Dense(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      4\u001b[0m activation1 \u001b[38;5;241m=\u001b[39m Activation_ReLU()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_data' is not defined"
     ]
    }
   ],
   "source": [
    "X,y = create_data(samples=100,classes=3)\n",
    "dense1 = Layer_Dense(2,64)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSProp\n",
    "class Optimizer_RMSprop:\n",
    "    def __init__(self,learning_rate=0.001,decay=0,epsilon=1e-7,rho=0.9):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "            layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "            layer.bas_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "            layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "            layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Optimizer_RMSprop object at 0x000001967F152DD0>\n"
     ]
    }
   ],
   "source": [
    "optimizer = Optimizer_RMSprop(decay=1e-4)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self,learning_rate=0.001,decay=0,epsilon=1e-7,beta_1=0.9,beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1./(1 + self.decay * self.iterations))\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        if not hasattr(layer,'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "            \n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "        layer.weight_momentums + \\\n",
    "        (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "        layer.bias_momentums + \\\n",
    "        (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "        (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "        (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "        (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "        (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "        (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "        (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "        weight_momentums_corrected / \\\n",
    "        (np.sqrt(weight_cache_corrected) +\n",
    "        self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "        bias_momentums_corrected / \\\n",
    "        (np.sqrt(bias_cache_corrected) +\n",
    "        self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.367, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.687, loss: 0.750, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.733, loss: 0.599, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.770, loss: 0.505, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.797, loss: 0.436, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.837, loss: 0.393, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.843, loss: 0.362, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.860, loss: 0.343, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.857, loss: 0.335, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.843, loss: 0.315, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.860, loss: 0.311, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.853, loss: 0.294, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.853, loss: 0.288, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.860, loss: 0.282, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.857, loss: 0.278, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.863, loss: 0.278, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.860, loss: 0.274, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.863, loss: 0.276, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.877, loss: 0.279, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.870, loss: 0.264, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.863, loss: 0.264, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.880, loss: 0.254, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.880, loss: 0.252, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.883, loss: 0.246, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.887, loss: 0.245, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.873, loss: 0.249, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.873, loss: 0.249, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.873, loss: 0.252, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.887, loss: 0.235, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.893, loss: 0.232, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.893, loss: 0.230, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.877, loss: 0.229, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.877, loss: 0.229, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.883, loss: 0.226, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.903, loss: 0.225, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.880, loss: 0.224, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.877, loss: 0.231, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.890, loss: 0.221, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.893, loss: 0.217, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.880, loss: 0.220, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.890, loss: 0.217, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.877, loss: 0.218, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.893, loss: 0.214, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.877, loss: 0.216, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.887, loss: 0.214, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.883, loss: 0.215, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.880, loss: 0.213, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.853, loss: 0.303, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.907, loss: 0.209, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.910, loss: 0.208, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.910, loss: 0.207, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.900, loss: 0.211, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.893, loss: 0.212, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.893, loss: 0.211, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.893, loss: 0.211, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.887, loss: 0.211, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.887, loss: 0.210, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.910, loss: 0.208, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.897, loss: 0.209, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.900, loss: 0.209, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.897, loss: 0.209, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.893, loss: 0.211, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.897, loss: 0.209, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.903, loss: 0.208, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.913, loss: 0.202, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.907, loss: 0.217, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.873, loss: 0.220, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.897, loss: 0.209, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.913, loss: 0.204, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.910, loss: 0.201, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.887, loss: 0.207, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.913, loss: 0.203, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.903, loss: 0.201, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.913, loss: 0.200, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.910, loss: 0.199, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.897, loss: 0.202, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.897, loss: 0.205, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.900, loss: 0.204, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.883, loss: 0.209, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.907, loss: 0.208, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.887, loss: 0.205, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.907, loss: 0.203, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.903, loss: 0.199, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.913, loss: 0.197, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.907, loss: 0.202, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.890, loss: 0.205, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.907, loss: 0.207, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.900, loss: 0.208, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.900, loss: 0.204, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.900, loss: 0.216, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.910, loss: 0.197, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.917, loss: 0.197, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.910, loss: 0.210, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.897, loss: 0.203, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.920, loss: 0.198, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.920, loss: 0.196, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.893, loss: 0.200, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.903, loss: 0.207, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.880, loss: 0.219, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.880, loss: 0.204, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.873, loss: 0.276, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "#Full code up to this point (from the github)\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dense layer Updated with the L1 and L2 regularization penalty \n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_L2=0,weight_regularizer_L1=0,bias_regularizer_L1=0,bias_regularizer_L2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        #set regularization strength\n",
    "        \n",
    "        self.weight_regularizer_l1 = weight_regularizer_L1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_L2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_L1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_L2\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "#same Zoom in the Loss_CategoricalCrossentropy class that will allow us to implement that L1 and L2 penalty\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "    def regularization_loss(self,layer):\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        \n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "         \n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases) \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, -1, 1], [1, -1, 1, -1], [-1, -1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#L1 derivation for the backward pass in plain python : \n",
    "weights = [[0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "dL1 = []\n",
    "for neuron in weights:\n",
    "    neuron_DL1 = []\n",
    "    \n",
    "    for weight in neuron:\n",
    "        if weight >= 0:\n",
    "            neuron_DL1.append(1)\n",
    "        else:\n",
    "            neuron_DL1.append(-1)\n",
    "    dL1.append(neuron_DL1) \n",
    "\n",
    "print(dL1)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.  1.]\n",
      " [ 1. -1.  1. -1.]\n",
      " [-1. -1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#using numpy\n",
    "import numpy as np\n",
    "\n",
    "weights = np.array([[0.2,0.8,-0.5,1],\n",
    "                    [0.5,-0.91,0.26,-0.5],\n",
    "                    [-0.26,-0.27,0.17,0.87]])\n",
    "\n",
    "dL1 = np.ones_like(weights)\n",
    "\n",
    "dL1[weights < 0] = -1\n",
    "print(dL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        #Gradients on regularization\n",
    "        #L1 on weights\n",
    "        \n",
    "        if self.weight_regularizer_11 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "            \n",
    "            #L2 on weights\n",
    "            \n",
    "            if self.weight_regularizer_l2 > 0:\n",
    "                self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "            if self.bias_regularizer_l1 > 0:\n",
    "                dL1 = np.ones_like(self.biases)\n",
    "                dL1[self.biases < 0] = -1\n",
    "                self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "            \n",
    "            if self.bias_regularizer_l2 > 0:\n",
    "                self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "            \n",
    "            self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "            \n",
    "            print(f'epoch : {epoch},' +\n",
    "                  f'acc: {accuracy:.3f},' +\n",
    "                  f'loss : {loss:.3f}' +\n",
    "                  f'data_loss {data_loss:.3f}' +\n",
    "                  f'reg_loss :  {regularization_loss:.3f}' +\n",
    "                  f'lr: {optimizer.current_learning_rate}')\n",
    "               \n",
    "\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.763, loss: 1.240\n"
     ]
    }
   ],
   "source": [
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_a = 'validation'\n",
    "y_test= 26.0\n",
    "prep_b = np.dot([1,1,1,1],[0.20,1.50,2.30,2.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -1.03, 0.67, 0, 0, -0.37, 0, 1.13, -0.07, 0]\n"
     ]
    }
   ],
   "source": [
    "#Lets use the dropout layer :\n",
    "\n",
    "import random \n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "example_output = [0.27, -1.03, 0.67, 0.99, 0.05,\n",
    "-0.37, -2.01, 1.13, -0.07, 0.73]\n",
    "\n",
    "while True :\n",
    "    \n",
    "    #Randomly choose index and set value to 0 \n",
    "    index = random.randint(0, len(example_output) - 1)\n",
    "    example_output[index] = 0\n",
    "    \n",
    "    #We might set an index that already is zeroed\n",
    "    #There are different ways of overcoming this problem,\n",
    "    #for simplicity we count values that are exactly 0 \n",
    "    #while its extremely rare in real model that weights are exactly 0, this is not the best method for sure\n",
    "    dropped_out = 0\n",
    "    for value in example_output:\n",
    "        if value ==0:\n",
    "            dropped_out += 1\n",
    "    #IF required number of outputs is zeroed - leave the loop\n",
    "    if dropped_out / len(example_output) >= dropout_rate:\n",
    "        break\n",
    " \n",
    "#We deactivate neurons in that part of the code    \n",
    "print(example_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 2, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code above is rudimental but we can dos that with The Binomial ditribution\n",
    "#Where each neurons have a 0.5 probability of being multiplied by 1 or by 0\n",
    "import numpy as np \n",
    "np.random.binomial(2,0.5,size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_rate = 0.20\n",
    "np.random.binomial(1,1-dropout_rate, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   -1.03  0.67  0.99  0.   -0.37 -2.01  1.13 -0.    0.73]\n"
     ]
    }
   ],
   "source": [
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05,\n",
    "-0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dropout_rate =0.3\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05,\n",
    "-0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "\n",
    "example_output *= np.random.binomial(1, 1-dropout_rate,example_output.shape)\n",
    "\n",
    "print(example_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum initial 0.36000000000000015\n",
      "mean sum : 0.3370062500000002\n"
     ]
    }
   ],
   "source": [
    "#To resolve the problem of the dropout so that we can use it during training and not during predictions\n",
    "#Here is the solution : \n",
    "\n",
    "example_output *= np.random.binomial(1,1-dropout_rate,example_output.shape) / (1 - dropout_rate)\n",
    "import numpy as np \n",
    "\n",
    "dropout_rate = 0.2\n",
    "example_output = np.array([0.27,-1.03,0.67,0.99,0.05,-0.37,-2.01,1.13,-0.07,0.73])\n",
    "print( f'sum initial {sum(example_output)}')\n",
    "\n",
    "sums = []\n",
    "\n",
    "for i in range(10000):\n",
    "    example_output2 = example_output * np.random.binomial(1,1-dropout_rate,example_output.shape) / (1 - dropout_rate)\n",
    "    sums.append(sum(example_output2))\n",
    "print(f'mean sum : {np.mean(sums)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Layer_Dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dvalues):\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs \u001b[38;5;241m=\u001b[39m dvalues \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_mask\n\u001b[1;32m---> 23\u001b[0m dense1 \u001b[38;5;241m=\u001b[39m \u001b[43mLayer_Dense\u001b[49m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m64\u001b[39m, weight_regularizer_l2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m,bias_regularizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)   \n\u001b[0;32m     24\u001b[0m activation1 \u001b[38;5;241m=\u001b[39m Activation_ReLU()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#Create dropout layer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Layer_Dense' is not defined"
     ]
    }
   ],
   "source": [
    "class Layer_Dropout:\n",
    "    \n",
    "    def __init__(self, rate):\n",
    "        \n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        #Save inputs values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        #Apply mask to output values\n",
    "        \n",
    "        self.output = inputs * self.binary_mask\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    " \n",
    "   \n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,bias_regularizer=5e-4)   \n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "#Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layers here ) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.104, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.05\n",
      "epoch: 100, acc: 0.932, loss: 0.186 (data_loss: 0.172, reg_loss: 0.014), lr: 0.04975371909050202\n",
      "epoch: 200, acc: 0.930, loss: 0.182 (data_loss: 0.174, reg_loss: 0.008), lr: 0.049507401356502806\n",
      "epoch: 300, acc: 0.933, loss: 0.179 (data_loss: 0.173, reg_loss: 0.007), lr: 0.0492635105177595\n",
      "epoch: 400, acc: 0.932, loss: 0.175 (data_loss: 0.169, reg_loss: 0.005), lr: 0.04902201088288642\n",
      "epoch: 500, acc: 0.936, loss: 0.178 (data_loss: 0.173, reg_loss: 0.005), lr: 0.048782867456949125\n",
      "epoch: 600, acc: 0.934, loss: 0.175 (data_loss: 0.170, reg_loss: 0.005), lr: 0.04854604592455945\n",
      "epoch: 700, acc: 0.934, loss: 0.176 (data_loss: 0.171, reg_loss: 0.005), lr: 0.048311512633460556\n",
      "epoch: 800, acc: 0.937, loss: 0.174 (data_loss: 0.169, reg_loss: 0.005), lr: 0.04807923457858551\n",
      "epoch: 900, acc: 0.933, loss: 0.176 (data_loss: 0.171, reg_loss: 0.005), lr: 0.04784917938657352\n",
      "epoch: 1000, acc: 0.929, loss: 0.180 (data_loss: 0.176, reg_loss: 0.004), lr: 0.04762131530072861\n",
      "epoch: 1100, acc: 0.936, loss: 0.172 (data_loss: 0.167, reg_loss: 0.005), lr: 0.04739561116640599\n",
      "epoch: 1200, acc: 0.934, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.04717203641681212\n",
      "epoch: 1300, acc: 0.933, loss: 0.173 (data_loss: 0.169, reg_loss: 0.004), lr: 0.04695056105920466\n",
      "epoch: 1400, acc: 0.934, loss: 0.172 (data_loss: 0.168, reg_loss: 0.004), lr: 0.04673115566147951\n",
      "epoch: 1500, acc: 0.936, loss: 0.174 (data_loss: 0.170, reg_loss: 0.004), lr: 0.046513791339132055\n",
      "epoch: 1600, acc: 0.939, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.04629843974258068\n",
      "epoch: 1700, acc: 0.935, loss: 0.172 (data_loss: 0.168, reg_loss: 0.004), lr: 0.046085073044840774\n",
      "epoch: 1800, acc: 0.932, loss: 0.174 (data_loss: 0.170, reg_loss: 0.004), lr: 0.04587366392953806\n",
      "epoch: 1900, acc: 0.932, loss: 0.173 (data_loss: 0.169, reg_loss: 0.004), lr: 0.04566418557925019\n",
      "epoch: 2000, acc: 0.932, loss: 0.173 (data_loss: 0.170, reg_loss: 0.004), lr: 0.045456611664166556\n",
      "epoch: 2100, acc: 0.934, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.045250916331055706\n",
      "epoch: 2200, acc: 0.937, loss: 0.173 (data_loss: 0.169, reg_loss: 0.004), lr: 0.0450470741925312\n",
      "epoch: 2300, acc: 0.933, loss: 0.176 (data_loss: 0.172, reg_loss: 0.004), lr: 0.04484506031660612\n",
      "epoch: 2400, acc: 0.935, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.04464485021652753\n",
      "epoch: 2500, acc: 0.935, loss: 0.173 (data_loss: 0.169, reg_loss: 0.004), lr: 0.044446419840881816\n",
      "epoch: 2600, acc: 0.936, loss: 0.172 (data_loss: 0.169, reg_loss: 0.004), lr: 0.04424974556396301\n",
      "epoch: 2700, acc: 0.933, loss: 0.173 (data_loss: 0.169, reg_loss: 0.004), lr: 0.04405480417639544\n",
      "epoch: 2800, acc: 0.936, loss: 0.169 (data_loss: 0.166, reg_loss: 0.004), lr: 0.04386157287600334\n",
      "epoch: 2900, acc: 0.930, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.04367002925891961\n",
      "epoch: 3000, acc: 0.935, loss: 0.173 (data_loss: 0.169, reg_loss: 0.004), lr: 0.043480151310926564\n",
      "epoch: 3100, acc: 0.931, loss: 0.172 (data_loss: 0.169, reg_loss: 0.004), lr: 0.04329191739902161\n",
      "epoch: 3200, acc: 0.934, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.043105306263201\n",
      "epoch: 3300, acc: 0.937, loss: 0.170 (data_loss: 0.166, reg_loss: 0.004), lr: 0.0429202970084553\n",
      "epoch: 3400, acc: 0.936, loss: 0.171 (data_loss: 0.168, reg_loss: 0.004), lr: 0.04273686909696996\n",
      "epoch: 3500, acc: 0.932, loss: 0.172 (data_loss: 0.169, reg_loss: 0.004), lr: 0.04255500234052514\n",
      "epoch: 3600, acc: 0.937, loss: 0.170 (data_loss: 0.166, reg_loss: 0.004), lr: 0.042374676893088686\n",
      "epoch: 3700, acc: 0.935, loss: 0.171 (data_loss: 0.167, reg_loss: 0.004), lr: 0.042195873243596776\n",
      "epoch: 3800, acc: 0.937, loss: 0.170 (data_loss: 0.166, reg_loss: 0.004), lr: 0.04201857220891634\n",
      "epoch: 3900, acc: 0.935, loss: 0.170 (data_loss: 0.166, reg_loss: 0.004), lr: 0.041842754926984395\n",
      "epoch: 4000, acc: 0.932, loss: 0.172 (data_loss: 0.168, reg_loss: 0.004), lr: 0.04166840285011875\n",
      "epoch: 4100, acc: 0.935, loss: 0.170 (data_loss: 0.167, reg_loss: 0.004), lr: 0.041495497738495375\n",
      "epoch: 4200, acc: 0.933, loss: 0.172 (data_loss: 0.168, reg_loss: 0.003), lr: 0.041324021653787346\n",
      "epoch: 4300, acc: 0.934, loss: 0.172 (data_loss: 0.168, reg_loss: 0.004), lr: 0.041153956952961035\n",
      "epoch: 4400, acc: 0.934, loss: 0.169 (data_loss: 0.165, reg_loss: 0.004), lr: 0.040985286282224684\n",
      "epoch: 4500, acc: 0.933, loss: 0.172 (data_loss: 0.168, reg_loss: 0.003), lr: 0.04081799257112535\n",
      "epoch: 4600, acc: 0.935, loss: 0.170 (data_loss: 0.167, reg_loss: 0.003), lr: 0.04065205902678971\n",
      "epoch: 4700, acc: 0.934, loss: 0.170 (data_loss: 0.166, reg_loss: 0.004), lr: 0.04048746912830479\n",
      "epoch: 4800, acc: 0.934, loss: 0.172 (data_loss: 0.168, reg_loss: 0.003), lr: 0.04032420662123473\n",
      "epoch: 4900, acc: 0.936, loss: 0.170 (data_loss: 0.167, reg_loss: 0.003), lr: 0.04016225551226957\n",
      "epoch: 5000, acc: 0.936, loss: 0.173 (data_loss: 0.170, reg_loss: 0.003), lr: 0.04000160006400256\n",
      "epoch: 5100, acc: 0.936, loss: 0.174 (data_loss: 0.170, reg_loss: 0.003), lr: 0.039842224789832265\n",
      "epoch: 5200, acc: 0.936, loss: 0.168 (data_loss: 0.165, reg_loss: 0.003), lr: 0.03968411444898608\n",
      "epoch: 5300, acc: 0.933, loss: 0.170 (data_loss: 0.166, reg_loss: 0.003), lr: 0.03952725404166173\n",
      "epoch: 5400, acc: 0.934, loss: 0.170 (data_loss: 0.166, reg_loss: 0.003), lr: 0.03937162880428363\n",
      "epoch: 5500, acc: 0.933, loss: 0.173 (data_loss: 0.170, reg_loss: 0.004), lr: 0.03921722420487078\n",
      "epoch: 5600, acc: 0.934, loss: 0.171 (data_loss: 0.168, reg_loss: 0.003), lr: 0.03906402593851323\n",
      "epoch: 5700, acc: 0.935, loss: 0.171 (data_loss: 0.167, reg_loss: 0.003), lr: 0.038912019922954205\n",
      "epoch: 5800, acc: 0.934, loss: 0.172 (data_loss: 0.168, reg_loss: 0.003), lr: 0.038761192294274965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 580\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    579\u001b[0m loss_activation\u001b[38;5;241m.\u001b[39mbackward(loss_activation\u001b[38;5;241m.\u001b[39moutput, y)\n\u001b[1;32m--> 580\u001b[0m \u001b[43mdense2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_activation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdinputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m dropout1\u001b[38;5;241m.\u001b[39mbackward(dense2\u001b[38;5;241m.\u001b[39mdinputs)\n\u001b[0;32m    582\u001b[0m activation1\u001b[38;5;241m.\u001b[39mbackward(dropout1\u001b[38;5;241m.\u001b[39mdinputs)\n",
      "Cell \u001b[1;32mIn[15], line 57\u001b[0m, in \u001b[0;36mLayer_Dense.backward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbiases \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_regularizer_l2 \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m     54\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Gradient on values\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                   np.sum(layer.weights * \\\n",
    "                                          layer.weights)\n",
    "\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                   np.sum(layer.biases * \\\n",
    "                                          layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_data(samples=1000, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through Dropout layer\n",
    "    dropout1.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "    \n",
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m,dvalues):\n\u001b[0;32m     13\u001b[0m         \n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m#Derivative - calculates from outpt of the sigmoid function : \u001b[39;00m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs \u001b[38;5;241m=\u001b[39m dvalues \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m---> 17\u001b[0m sample_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[43my_true\u001b[49m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(y_pred) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_pred))       \n\u001b[0;32m     18\u001b[0m sample_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(sample_losses, axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[0;32m     21\u001b[0m                     [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m],\n\u001b[0;32m     22\u001b[0m                     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m     23\u001b[0m                     [\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m],\n\u001b[0;32m     24\u001b[0m                     [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "#Chapter 16 Binary Logistic Regression\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    \n",
    "    #Forward pass:\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        #Save input and calculate/save output\n",
    "        #of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1/(1 + np.exp(-inputs))\n",
    "    def backward(self,dvalues):\n",
    "        \n",
    "        #Derivative - calculates from outpt of the sigmoid function : \n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "sample_losses = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))       \n",
    "sample_losses = np.mean(sample_losses, axis =-1)\n",
    "\n",
    "outputs = np.array([[1, 2, 3],\n",
    "                    [2, 4, 6],\n",
    "                    [0, 5, 10],\n",
    "                    [11, 12, 13],\n",
    "                    [5, 10, 15]])\n",
    "np.mean(outputs, axis =- 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  4.,  5., 12., 10.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "outputs = np.array([[1, 2, 3],\n",
    "                    [2, 4, 6],\n",
    "                    [0, 5, 10],\n",
    "                    [11, 12, 13],\n",
    "                    [5, 10, 15]])\n",
    "np.mean(outputs, axis =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Creating the Sine Dataset\n",
    "# Sine sample dataset\n",
    "def create_data(samples=1000):\n",
    "\n",
    "    X = np.arange(samples).reshape(-1, 1) / samples\n",
    "    y = np.sin(2 * np.pi * X).reshape(-1, 1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "X, y = create_data()\n",
    "plt.plot(X,y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear activation : \n",
    "class Activation_Linear:\n",
    "    \n",
    "    #Forward pass\n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        #Just rememeber values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        #derivative is 1, 1 * dvalues = dvalues - chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Squared Error Loss : \n",
    "class Loss:\n",
    "    pass\n",
    "\n",
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    def forward(self,y_pred,y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        return sample_losses\n",
    "    \n",
    "    \n",
    "    #Backward pass\n",
    "    def backward(self,dvalues,y_true):\n",
    "        \n",
    "        #Number of samples\n",
    "        samples =  len(dvalues)\n",
    "        \n",
    "        #Number of outputs in every sample\n",
    "        #We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        #Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        #Normalize gradient :\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Absolute Error Loss Code\n",
    "\n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    def forward(self, y_pred,y_true):\n",
    "        \n",
    "        #Calculate loss:\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis =-1)\n",
    "        \n",
    "        #Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self,dvalues,y_true):\n",
    "        \n",
    "        #Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #Number of outputs in every sample\n",
    "        #We'll use the first sample to count them\n",
    "        \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        #Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        #Normalize gradient\n",
    "        self.dinputs = self.dinputs /samples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are trying to get the accuracy of a regression model : \n",
    "\n",
    "accuracy_precision = np.std(y) / 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = activation2.output\n",
    "\n",
    "accuracy = np.mean(np.absolte(predictions - y ) < accuracy_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Layer_Dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#We will create our model with a new activation function here : Sigmoid, our loss (mean squared or mean error)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X, y \u001b[38;5;241m=\u001b[39m create_data()\n\u001b[1;32m----> 4\u001b[0m dense1 \u001b[38;5;241m=\u001b[39m \u001b[43mLayer_Dense\u001b[49m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      6\u001b[0m activation1 \u001b[38;5;241m=\u001b[39m Activation_ReLU()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Create a second Dense Layer with 64 input features (as we take output \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# of previous layer here ) and a1 output value\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Layer_Dense' is not defined"
     ]
    }
   ],
   "source": [
    "#We will create our model with a new activation function here : Sigmoid, our loss (mean squared or mean error)\n",
    "X, y = create_data()\n",
    "\n",
    "dense1 = Layer_Dense(1,64)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#Create a second Dense Layer with 64 input features (as we take output \n",
    "# of previous layer here ) and a1 output value\n",
    "\n",
    "dense2 = Layer_Dense(64,1)\n",
    "\n",
    "#Create a Linear Activation\n",
    "activation2 = Activation_Linear()\n",
    "\n",
    "#Create loss function \n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "#Create otimizer\n",
    "optimizer = Optimizer_Adam()\n",
    "\n",
    "\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    data_loss = loss_function.calculate(activation2.output, y )\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + \\\n",
    "                          loss_function.regularization_loss(dense2)\n",
    "    \n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    predictions = activation2.output\n",
    "    \n",
    "    accuracy = np.mean(np.absolute(predictions - y ) < accuracy_precision)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch : {epoch} , ' +\n",
    "              f'acc : {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    #backward pass\n",
    "    loss_function.backward(activation2.output, y )\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    #update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Might be time for an exercice were we need to use all of our brain :\n",
    "#We will need to create a model from scratch (exeption can be done of getting\n",
    "# the derivative out on the internet)\n",
    "#that fits perfectly the sine function (first we will thing about it as a Classification Problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
